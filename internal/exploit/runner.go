package exploit

import (
	"context"
	"errors"
	"fmt"
	"os"
	"os/exec"
	"sync"
	"time"

	"neo/internal/client"
	"neo/internal/config"
	"neo/internal/queue"
	"neo/pkg/tasklogger"

	"github.com/google/go-cmp/cmp"
	"github.com/google/go-cmp/cmp/cmpopts"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/push"
	"github.com/sirupsen/logrus"

	neopb "neo/lib/genproto/neo"
)

var (
	ErrFinishedUnexpectedly = errors.New("service finished unexpectedly")
)

func NewRunner(
	clientID string,
	maxJobs, maxEndlessJobs int,
	clientConfig *client.Config,
	c *client.Client,
	logSender tasklogger.Sender,
) *Runner {
	return &Runner{
		storage:        NewStorage(NewCache(), clientConfig.ExploitDir, c),
		cfg:            &config.Config{},
		client:         c,
		maxJobs:        maxJobs,
		maxEndlessJobs: maxEndlessJobs,
		singleRuns:     make(chan *neopb.SingleRunRequest),
		restarts:       make(chan struct{}, 1),
		logSender:      logSender,
		metricsPusher: push.
			New(clientConfig.MetricsHost, "neo_runner").
			Grouping("client_id", clientID).
			Gatherer(prometheus.DefaultGatherer),
		logger: logrus.WithField("component", "exploit-runner"),
	}
}

type Runner struct {
	storage        *Storage
	teams          map[string]string
	cfg            *config.Config
	client         *client.Client
	logSender      tasklogger.Sender
	metricsPusher  *push.Pusher
	maxJobs        int
	maxEndlessJobs int

	singleRuns chan *neopb.SingleRunRequest
	restarts   chan struct{}

	logger *logrus.Entry
}

func (r *Runner) Run(ctx context.Context) error {
	r.printGreeting()

	state, err := r.pingHeartbeat(ctx)
	if err != nil {
		return fmt.Errorf("sending heartbeat: %w", err)
	}
	if err := r.onServerStateUpdate(ctx, state); err != nil {
		return fmt.Errorf("on server state update: %w", err)
	}

	wg := sync.WaitGroup{}

	srvCtx, srvCancel := context.WithCancel(ctx)
	defer srvCancel()

	// No need to fail here as these retry loops are endless.
	wg.Add(3)
	go func() {
		defer wg.Done()
		if err := retryLoop(srvCtx, "broadcast", -1, 3*time.Second, r.listenBroadcast); err != nil && !errors.Is(err, context.Canceled) {
			r.logger.WithError(err).Error("Error running broadcast loop")
		}
	}()
	go func() {
		defer wg.Done()
		if err := retryLoop(srvCtx, "single runs", -1, 3*time.Second, r.listenSingleRuns); err != nil && !errors.Is(err, context.Canceled) {
			r.logger.WithError(err).Errorf("Error running single runs loop")
		}
	}()
	go func() {
		defer wg.Done()
		r.startPushingMetrics(srvCtx)
	}()
	defer func() {
		r.logger.Info("Stopping aux services")
		srvCancel()
		r.logger.Info("Waiting for aux services to stop")
		wg.Wait()
	}()

	r.eventLoop(ctx)

	r.logger.Info("Worker is finishing, sending the leave ping")
	leaveCtx, cancel := context.WithTimeout(context.Background(), time.Second*5)
	defer cancel()
	if _, err := r.client.Ping(leaveCtx, neopb.PingRequest_LEAVE); err != nil {
		return fmt.Errorf("failed to send leave ping: %w", err)
	}

	return nil
}

func (r *Runner) eventLoop(ctx context.Context) {
	defer r.logger.Info("Event loop stopped")

	var (
		simpleLoop  *submitLoop
		endlessLoop *submitLoop

		loopsWG     sync.WaitGroup
		loopsCtx    context.Context
		loopsCancel context.CancelFunc
	)

	startLoops := func() {
		simpleLoop = newSubmitLoop(r.maxJobs, r.cfg, queue.NewSimpleQueue)
		endlessLoop = newSubmitLoop(r.maxJobs, r.cfg, queue.NewEndlessQueue)

		loopsCtx, loopsCancel = context.WithCancel(ctx)
		r.logger.Info("Starting submit loops")
		loopsWG.Add(2)
		go func() {
			defer loopsWG.Done()
			simpleLoop.Start(loopsCtx)
			r.logger.Info("Simple submit loop finished")
		}()
		go func() {
			defer loopsWG.Done()
			endlessLoop.Start(loopsCtx)
			r.logger.Info("Endless submit loop finished")
		}()
	}
	startLoops()

	stopLoops := func() {
		r.logger.Info("Stopping submit loops")
		loopsCancel()
		r.logger.Info("Waiting for submit loops to stop")
		loopsWG.Wait()
	}
	defer stopLoops()

	r.scheduleEndlessExploits(endlessLoop)

	runTicker := time.NewTicker(time.Second)
	defer runTicker.Stop()

	savedPingEvery := r.cfg.PingEvery
	pingTicker := time.NewTicker(savedPingEvery)

	for {
		select {
		case <-pingTicker.C:
			r.logger.Debug("Sending ping heartbeat")
			state, err := r.pingHeartbeat(ctx)
			if err != nil {
				r.logger.WithError(err).Error("Error sending recurrent heartbeat")
			}
			if err := r.onServerStateUpdate(ctx, state); err != nil {
				r.logger.WithError(err).Error("Error processing server state update")
			} else if r.cfg.PingEvery != savedPingEvery {
				savedPingEvery = r.cfg.PingEvery
				pingTicker.Reset(savedPingEvery)
			}

		case <-r.restarts:
			r.logger.Info("Got loops restart request")
			stopLoops()
			startLoops()
			r.scheduleEndlessExploits(endlessLoop)

		case <-runTicker.C:
			r.logger.Debug("Scheduling recurrent exploits")
			r.scheduleRecurrentExploits(simpleLoop)

		case req := <-r.singleRuns:
			r.logger.Infof("Processing single run request %v", req)

			// Update config to guarantee the requested exploit exists locally.
			if _, err := r.pingHeartbeat(ctx); err != nil {
				r.logger.WithError(err).Error("Error updating config before single run")
			}
			if err := r.submitExploit(simpleLoop, req.ExploitId); err != nil {
				r.logger.WithError(err).Errorf("Error submitting single run exploit %v", req.ExploitId)
			}

		case <-ctx.Done():
			return
		}
	}
}

func (r *Runner) pingHeartbeat(ctx context.Context) (*neopb.ServerState, error) {
	state, err := r.client.Ping(ctx, neopb.PingRequest_HEARTBEAT)
	if err != nil {
		return nil, fmt.Errorf("sending ping request: %w", err)
	}
	return state, nil
}

func (r *Runner) scheduleRecurrentExploits(l *submitLoop) {
	exs := r.storage.Exploits()
	now := time.Now()
	for _, ex := range exs {
		if ex.Disabled || ex.Endless {
			continue
		}
		if ex.LastRun.Add(ex.RunEvery).After(now) {
			continue
		}
		if err := r.submitExploit(l, ex.ID); err != nil {
			r.logger.WithError(err).Errorf("Error submitting recurrent exploit %v", ex.ID)
		}
	}
}

func (r *Runner) scheduleEndlessExploits(l *submitLoop) {
	exs := r.storage.Exploits()
	for _, ex := range exs {
		if ex.Disabled || !ex.Endless {
			continue
		}
		if err := r.submitExploit(l, ex.ID); err != nil {
			r.logger.WithError(err).Errorf("Error submitting endless exploit %v, restarting loops", ex.ID)
			// Need to restart loops here as otherwise endless exploits will be stuck endlessly in the queue.
			// This also provides an out-of-the-box retry mechanism for endless exploits,
			// which is needed to detect problems from the logs.
			r.restartLoops()
		}
	}
}

func (r *Runner) submitExploit(l *submitLoop, id string) error {
	ex, ok := r.storage.Exploit(id)
	if !ok {
		return fmt.Errorf("exploit %v not found", id)
	}

	tasks := CreateExploitTasks(ex, r.teams, r.cfg.Environ, r.logSender)
	for _, t := range tasks {
		r.logger.Infof("Adding task: %+v", t)
		if err := l.Add(t); err != nil {
			return fmt.Errorf("adding task to queue: %w", err)
		}
	}

	r.storage.UpdateLastRun(ex.ID, time.Now())
	return nil
}

func (r *Runner) restartLoops() {
	select {
	case r.restarts <- struct{}{}:
	default:
	}
}

func (r *Runner) listenBroadcast(ctx context.Context) error {
	resp, err := r.client.ListenBroadcasts(ctx)
	if err != nil {
		return fmt.Errorf("making broadcasts listen request: %w", err)
	}
	for {
		select {
		case cmd, ok := <-resp:
			if !ok {
				r.logger.Error("Broadcast channel was closed, exiting")
				return ErrFinishedUnexpectedly
			}
			r.logger.Infof("Received a command from broadcast: %v", cmd)
			if err := r.handleBroadcastCommand(ctx, cmd); err != nil {
				r.logger.WithError(err).Error("Error running broadcast command")
			}
		case <-ctx.Done():
			return ctx.Err()
		}
	}
}

func (r *Runner) listenSingleRuns(ctx context.Context) error {
	resp, err := r.client.ListenSingleRuns(ctx)
	if err != nil {
		return fmt.Errorf("making single run listen request: %w", err)
	}
	for {
		select {
		case req, ok := <-resp:
			if !ok {
				r.logger.Error("Single run channel was closed, exiting")
				return ErrFinishedUnexpectedly
			}
			r.logger.Infof("Received a single run request: %v", req)
			r.singleRuns <- req
		case <-ctx.Done():
			return ctx.Err()
		}
	}
}

func (r *Runner) startPushingMetrics(ctx context.Context) {
	t := time.NewTicker(time.Second * 5)
	defer t.Stop()
	for {
		select {
		case <-t.C:
			if err := r.metricsPusher.PushContext(ctx); err != nil {
				r.logger.WithError(err).Error("Error pushing metrics")
			}
		case <-ctx.Done():
			return
		}
	}
}

func (r *Runner) handleBroadcastCommand(ctx context.Context, cmd *neopb.Command) error {
	c := exec.CommandContext(ctx, "/bin/bash", "-c", cmd.Command)
	c.Stdout = os.Stdout
	c.Stderr = os.Stderr
	if err := c.Run(); err != nil {
		return fmt.Errorf("executing command: %w", err)
	}
	return nil
}

func (r *Runner) onServerStateUpdate(ctx context.Context, state *neopb.ServerState) error {
	cfg, err := config.FromProto(state.Config)
	if err != nil {
		return fmt.Errorf("parsing config: %w", err)
	}
	r.cfg = cfg

	cid := r.client.ID
	if ipbuck, ok := state.ClientTeamMap[cid]; !ok {
		r.logger.Warning("Client is not in the team map")
	} else {
		if diff := cmp.Diff(r.teams, ipbuck.Teams, cmpopts.EquateEmpty()); diff != "" {
			r.teams = ipbuck.Teams
			r.logger.Info("Teams changed, scheduling loops restart")
			r.restartLoops()
		}
	}

	if r.storage.UpdateExploits(ctx, state.Exploits) {
		r.logger.Info("Exploits changed, scheduling loops restart")
		r.restartLoops()
	}

	return nil
}

func (r *Runner) printGreeting() {
	fmt.Println(`       _
   ___| |__  ___   _ __   ___  ___    _ __ _   _ _ __  _ __   ___ _ __
  / __| '_ \/ __| | '_ \ / _ \/ _ \  | '__| | | | '_ \| '_ \ / _ \ '__|
 | (__| |_) \__ \ | | | |  __/ (_) | | |  | |_| | | | | | | |  __/ |
  \___|_.__/|___/ |_| |_|\___|\___/  |_|   \__,_|_| |_|_| |_|\___|_|
                                                                       `)
}

func CreateExploitTasks(
	ex *State,
	teams map[string]string,
	environ []string,
	sender tasklogger.Sender,
) (tasks []*queue.Task) {
	for id, ip := range teams {
		logger := tasklogger.New(ex.ID, ex.Version, ip, sender)
		tasks = append(tasks, queue.NewTask(ex.ID, ex.Path, ex.Dir, id, ip, environ, ex.Timeout, logger))
	}
	return
}
