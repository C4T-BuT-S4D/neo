package exploit

import (
	"context"
	"errors"
	"fmt"
	"os"
	"os/exec"
	"sync"
	"time"

	"github.com/google/go-cmp/cmp"
	"github.com/google/go-cmp/cmp/cmpopts"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/push"
	"github.com/prometheus/common/expfmt"
	"github.com/sirupsen/logrus"

	"github.com/c4t-but-s4d/neo/v2/internal/client"
	"github.com/c4t-but-s4d/neo/v2/internal/config"
	"github.com/c4t-but-s4d/neo/v2/internal/models"
	"github.com/c4t-but-s4d/neo/v2/internal/queue"
	"github.com/c4t-but-s4d/neo/v2/pkg/joblogger"
	epb "github.com/c4t-but-s4d/neo/v2/proto/go/exploits"
)

var (
	ErrFinishedUnexpectedly = errors.New("service finished unexpectedly")
)

func NewRunner(
	clientID string,
	maxJobs, maxEndlessJobs int,
	clientConfig *client.Config,
	c *client.Client,
	logSender joblogger.Sender,
) *Runner {
	return &Runner{
		storage:        NewStorage(NewCache(), clientConfig.ExploitDir, c),
		cfg:            &config.ExploitsConfig{},
		client:         c,
		maxJobs:        maxJobs,
		maxEndlessJobs: maxEndlessJobs,
		singleRuns:     make(chan *epb.SingleRunSubscribeResponse),
		restarts:       make(chan struct{}, 1),
		logSender:      logSender,
		metricsPusher: push.
			New(clientConfig.MetricsHost, "neo_runner").
			Grouping("client_id", clientID).
			Format(expfmt.FmtOpenMetrics_1_0_0).
			Gatherer(prometheus.DefaultGatherer),
		metrics: NewMetrics("neo"),
		logger:  logrus.WithField("component", "exploit-runner"),
	}
}

type Runner struct {
	storage       *Storage
	teams         map[string]string
	cfg           *config.ExploitsConfig
	client        *client.Client
	logSender     joblogger.Sender
	metricsPusher *push.Pusher
	metrics       *Metrics

	maxJobs        int
	maxEndlessJobs int

	simpleLoop  *submitLoop
	endlessLoop *submitLoop

	singleRuns chan *epb.SingleRunSubscribeResponse
	restarts   chan struct{}

	logger *logrus.Entry
}

func (r *Runner) Run(ctx context.Context) error {
	r.printGreeting()

	state, err := r.pingHeartbeat(ctx)
	if err != nil {
		return fmt.Errorf("sending heartbeat: %w", err)
	}
	if err := r.onServerStateUpdate(ctx, state); err != nil {
		return fmt.Errorf("on server state update: %w", err)
	}

	// Drain updates.
	select {
	case <-r.restarts:
	default:
	}

	wg := sync.WaitGroup{}

	srvCtx, srvCancel := context.WithCancel(ctx)
	defer srvCancel()

	// No need to fail here as these retry loops are endless.
	wg.Add(3)
	go func() {
		defer wg.Done()
		if err := retryLoop(srvCtx, "broadcast", -1, 3*time.Second, r.listenBroadcast); err != nil && !errors.Is(err, context.Canceled) {
			r.logger.WithError(err).Error("Error running broadcast loop")
		}
	}()
	go func() {
		defer wg.Done()
		if err := retryLoop(srvCtx, "single runs", -1, 3*time.Second, r.listenSingleRuns); err != nil && !errors.Is(err, context.Canceled) {
			r.logger.WithError(err).Errorf("Error running single runs loop")
		}
	}()
	go func() {
		defer wg.Done()
		r.startPushingMetrics(srvCtx)
	}()
	defer func() {
		r.logger.Info("Stopping aux services")
		srvCancel()
		r.logger.Info("Waiting for aux services to stop")
		wg.Wait()
	}()

	r.eventLoop(ctx)

	r.logger.Info("Worker is finishing, sending the leave ping")
	leaveCtx, cancel := context.WithTimeout(context.Background(), time.Second*5)
	defer cancel()
	if err := r.client.Leave(leaveCtx); err != nil {
		return fmt.Errorf("failed to send leave: %w", err)
	}

	return nil
}

func (r *Runner) eventLoop(ctx context.Context) {
	defer r.logger.Info("Event loop stopped")

	var loopsWg sync.WaitGroup
	loopsCancel := r.runWorkers(ctx, &loopsWg)

	stopLoops := func() {
		r.logger.Info("Stopping submit loops")
		loopsCancel()
		r.logger.Info("Waiting for submit loops to stop")
		loopsWg.Wait()
	}
	defer stopLoops()

	r.scheduleEndlessExploits()

	runTicker := time.NewTicker(time.Second)
	defer runTicker.Stop()

	savedPingEvery := r.cfg.PingEvery
	pingTicker := time.NewTicker(savedPingEvery)

	for {
		select {
		case <-pingTicker.C:
			r.logger.Debug("Sending ping heartbeat")
			state, err := r.pingHeartbeat(ctx)
			if err != nil {
				r.logger.WithError(err).Error("Error sending recurrent heartbeat")
				return
			}
			if err := r.onServerStateUpdate(ctx, state); err != nil {
				r.logger.WithError(err).Error("Error processing server state update")
			} else if r.cfg.PingEvery != savedPingEvery {
				savedPingEvery = r.cfg.PingEvery
				pingTicker.Reset(savedPingEvery)
			}

		case <-r.restarts:
			r.logger.Info("Got loops restart request")
			stopLoops()
			loopsCancel = r.runWorkers(ctx, &loopsWg)
			r.scheduleEndlessExploits()

		case <-runTicker.C:
			r.logger.Debug("Scheduling recurrent exploits")
			r.scheduleRecurrentExploits()

		case req := <-r.singleRuns:
			r.logger.Infof("Processing single run request %v", req)

			// Update config to guarantee the requested exploit exists locally.
			if _, err := r.pingHeartbeat(ctx); err != nil {
				r.logger.WithError(err).Error("Error updating config before single run")
			}
			if err := r.submitExploit(r.simpleLoop, req.ExploitId); err != nil {
				r.logger.WithError(err).Errorf("Error submitting single run exploit %v", req.ExploitId)
			}

		case <-ctx.Done():
			return
		}
	}
}

func (r *Runner) pingHeartbeat(ctx context.Context) (*epb.ServerState, error) {
	state, err := r.client.Heartbeat(ctx)
	if err != nil {
		return nil, fmt.Errorf("sending heartbeat: %w", err)
	}
	return state, nil
}

func (r *Runner) runWorkers(ctx context.Context, wg *sync.WaitGroup) context.CancelFunc {
	r.simpleLoop = newSubmitLoop(r.maxJobs, r.cfg, r.metrics, queue.NewSimpleQueueFactory())
	r.endlessLoop = newSubmitLoop(r.maxEndlessJobs, r.cfg, r.metrics, queue.NewEndlessQueueFactory())

	loopsCtx, loopsCancel := context.WithCancel(ctx)
	r.logger.Info("Starting submit loops")
	wg.Add(2)
	go func() {
		defer wg.Done()
		r.simpleLoop.Start(loopsCtx)
		r.logger.Info("Simple submit loop finished")
	}()
	go func() {
		defer wg.Done()
		r.endlessLoop.Start(loopsCtx)
		r.logger.Info("Endless submit loop finished")
	}()
	return loopsCancel
}

func (r *Runner) scheduleRecurrentExploits() {
	exs := r.storage.Exploits()
	now := time.Now()
	for _, ex := range exs {
		if ex.Disabled || ex.Endless {
			continue
		}
		if ex.LastRun.Add(ex.RunEvery).After(now) {
			continue
		}
		if err := r.submitExploit(r.simpleLoop, ex.ID); err != nil {
			r.logger.WithError(err).Errorf("Error submitting recurrent exploit %v", ex.ID)
		}
	}
}

func (r *Runner) scheduleEndlessExploits() {
	exs := r.storage.Exploits()
	for _, ex := range exs {
		if ex.Disabled || !ex.Endless {
			continue
		}
		if err := r.submitExploit(r.endlessLoop, ex.ID); err != nil {
			r.logger.WithError(err).Errorf("Error submitting endless exploit %v, restarting loops", ex.ID)
			// Need to restart loops here as otherwise endless exploits will be stuck endlessly in the queue.
			// This also provides an out-of-the-box retry mechanism for endless exploits,
			// which is needed to detect problems from the logs.
			r.restartLoops()
		}
	}
}

func (r *Runner) submitExploit(l *submitLoop, id string) error {
	ex, ok := r.storage.Exploit(id)
	if !ok {
		return fmt.Errorf("exploit %v not found", id)
	}

	jobs := CreateExploitJobs(ex, r.teams, r.cfg.Environ, r.logSender)
	for _, t := range jobs {
		r.logger.Infof("Adding job: %v", t)
		if err := l.Add(t); err != nil {
			return fmt.Errorf("adding task to queue: %w", err)
		}
	}

	r.storage.UpdateLastRun(ex.ID, time.Now())
	return nil
}

func (r *Runner) restartLoops() {
	select {
	case r.restarts <- struct{}{}:
	default:
	}
}

func (r *Runner) listenBroadcast(ctx context.Context) error {
	resp, err := r.client.ListenBroadcasts(ctx)
	if err != nil {
		return fmt.Errorf("making broadcasts listen request: %w", err)
	}
	for {
		select {
		case cmd, ok := <-resp:
			if !ok {
				r.logger.Error("Broadcast channel was closed, exiting")
				return ErrFinishedUnexpectedly
			}
			r.logger.Infof("Received a command from broadcast: %v", cmd)
			if err := r.handleBroadcastCommand(ctx, cmd); err != nil {
				r.logger.WithError(err).Error("Error running broadcast command")
			}
		case <-ctx.Done():
			return ctx.Err()
		}
	}
}

func (r *Runner) listenSingleRuns(ctx context.Context) error {
	resp, err := r.client.ListenSingleRuns(ctx)
	if err != nil {
		return fmt.Errorf("making single run listen request: %w", err)
	}
	for {
		select {
		case req, ok := <-resp:
			if !ok {
				r.logger.Error("Single run channel was closed, exiting")
				return ErrFinishedUnexpectedly
			}
			r.logger.Infof("Received a single run request: %v", req)
			r.singleRuns <- req
		case <-ctx.Done():
			return ctx.Err()
		}
	}
}

func (r *Runner) startPushingMetrics(ctx context.Context) {
	t := time.NewTicker(time.Second * 1)
	defer t.Stop()
	for {
		select {
		case <-t.C:
			if err := r.metricsPusher.PushContext(ctx); err != nil {
				r.logger.WithError(err).Error("Error pushing metrics")
			}
		case <-ctx.Done():
			return
		}
	}
}

func (r *Runner) handleBroadcastCommand(ctx context.Context, cmd *epb.BroadcastSubscribeResponse) error {
	c := exec.CommandContext(ctx, "/bin/bash", "-c", cmd.Command)
	c.Stdout = os.Stdout
	c.Stderr = os.Stderr
	if err := c.Run(); err != nil {
		return fmt.Errorf("executing command: %w", err)
	}
	return nil
}

func (r *Runner) onServerStateUpdate(ctx context.Context, state *epb.ServerState) error {
	cfg, err := config.FromProto(state.Config)
	if err != nil {
		return fmt.Errorf("parsing config: %w", err)
	}
	r.cfg = cfg

	cid := r.client.ID
	if ipbuck, ok := state.ClientTeamMap[cid]; !ok {
		r.logger.Warning("Client is not in the team map")
	} else {
		if diff := cmp.Diff(r.teams, ipbuck.Teams, cmpopts.EquateEmpty()); diff != "" {
			r.teams = ipbuck.Teams
			r.logger.Info("Teams changed, scheduling loops restart")
			r.restartLoops()
		}
		r.metrics.Teams.Set(float64(len(r.teams)))
	}

	if r.storage.UpdateExploits(ctx, state.Exploits) {
		r.storage.ScaleTimeouts(r.maxJobs, len(r.teams))
		r.logger.Info("Exploits changed, scheduling loops restart")
		r.restartLoops()
	}

	return nil
}

func (r *Runner) printGreeting() {
	fmt.Println(`       _
   ___| |__  ___   _ __   ___  ___    _ __ _   _ _ __  _ __   ___ _ __
  / __| '_ \/ __| | '_ \ / _ \/ _ \  | '__| | | | '_ \| '_ \ / _ \ '__|
 | (__| |_) \__ \ | | | |  __/ (_) | | |  | |_| | | | | | | |  __/ |
  \___|_.__/|___/ |_| |_|\___|\___/  |_|   \__,_|_| |_|_| |_|\___|_|
                                                                       `)
}

func CreateExploitJobs(
	ex *State,
	teams map[string]string,
	environ []string,
	sender joblogger.Sender,
) (jobs []*queue.Job) {
	for id, ip := range teams {
		jobs = append(jobs, queue.NewJob(
			models.NewExploit(ex.ID, ex.Version, ex.ExploitType()),
			models.NewTarget(id, ip),
			ex.Path,
			ex.Dir,
			environ,
			ex.Timeout,
			joblogger.New(ex.ID, ex.Version, ip, sender),
		))
	}
	return
}

func retryLoop(ctx context.Context, name string, retries int, interval time.Duration, f func(ctx context.Context) error) error {
	t := time.NewTimer(interval)
	defer t.Stop()

	var err error
	for attempts := 0; retries == -1 || attempts < retries; attempts++ {
		logrus.Infof("Starting loop %s", name)
		err = f(ctx)
		if err == nil || errors.Is(err, context.Canceled) {
			logrus.Infof("Loop %s gracefully finishing", name)
			return err
		}
		if retries == -1 || attempts+1 < retries {
			logrus.Errorf("Loop %s returned error: %v, restarting", name, err)
			t.Reset(interval)
			select {
			case <-t.C:
			case <-ctx.Done():
				return ctx.Err()
			}
		}
	}
	return err
}
